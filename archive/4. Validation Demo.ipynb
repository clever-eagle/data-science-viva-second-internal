{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab89cb7b",
   "metadata": {},
   "source": [
    "# Machine Learning Validation Techniques: A Complete Guide\n",
    "\n",
    "## Why Validation Matters\n",
    "\n",
    "Imagine you're studying for an exam by only practicing with the same set of questions. You might memorize the answers perfectly, but fail when faced with new questions. This is exactly what happens when machine learning models **overfit** to training data.\n",
    "\n",
    "**Validation techniques** help us:\n",
    "- Estimate how well our model generalizes to unseen data\n",
    "- Detect overfitting early\n",
    "- Compare different models fairly\n",
    "- Tune hyperparameters without biasing our final evaluation\n",
    "\n",
    "In this notebook, we'll explore different validation strategies using the classic **Iris dataset** - a simple dataset perfect for understanding these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e0253",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "Let's start by importing our libraries and loading the Iris dataset, which contains measurements of 150 iris flowers from 3 different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe89398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(y, iris.target_names)\n",
    "\n",
    "print(\"Dataset Shape:\", X.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(10))\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb8ae0",
   "metadata": {},
   "source": [
    "## 2. Simple Train-Test Split\n",
    "\n",
    "### The Concept\n",
    "\n",
    "The simplest validation approach is to split your data into two parts:\n",
    "- **Training Set** (typically 70-80%): Used to train the model\n",
    "- **Test Set** (typically 20-30%): Used to evaluate the model\n",
    "\n",
    "**Analogy:** It's like studying from a textbook (training) and then taking a final exam with different questions (testing).\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and fast\n",
    "- Good for large datasets\n",
    "\n",
    "**Disadvantages:**\n",
    "- High variance in performance estimates (depends on which samples end up in test set)\n",
    "- Wastes data (test set isn't used for training)\n",
    "- Can be problematic with small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4697478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Test set size: {len(X_test)} samples\")\n",
    "\n",
    "# Train a simple logistic regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on both sets\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Difference: {abs(train_accuracy - test_accuracy):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81d8f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Visualize the train-test split\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Data distribution\n",
    "train_classes = np.bincount(y_train)\n",
    "test_classes = np.bincount(y_test)\n",
    "\n",
    "x_pos = np.arange(len(iris.target_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, train_classes, width, label='Training Set', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, test_classes, width, label='Test Set', alpha=0.8)\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Samples', fontsize=12)\n",
    "axes[0].set_title('Class Distribution: Train vs Test', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(iris.target_names)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy comparison\n",
    "accuracies = [train_accuracy, test_accuracy]\n",
    "labels = ['Training', 'Test']\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = axes[1].bar(labels, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Model Performance', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a20a3",
   "metadata": {},
   "source": [
    "## 3. K-Fold Cross-Validation\n",
    "\n",
    "### The Concept\n",
    "\n",
    "Instead of a single train-test split, K-Fold Cross-Validation divides the data into **K equal-sized folds** and performs K training iterations:\n",
    "\n",
    "1. Split data into K folds (e.g., K=5)\n",
    "2. For each iteration i (1 to K):\n",
    "   - Use fold i as the test set\n",
    "   - Use remaining K-1 folds as the training set\n",
    "   - Train and evaluate the model\n",
    "3. Average the K performance scores\n",
    "\n",
    "**Analogy:** Instead of one final exam, you take K different exams, each covering different material. Your final grade is the average.\n",
    "\n",
    "**Advantages:**\n",
    "- More reliable performance estimate (lower variance)\n",
    "- Every sample is used for both training and testing\n",
    "- Better for small datasets\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive (K times slower)\n",
    "- Still can have imbalanced folds in classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188aa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform K-Fold Cross-Validation\n",
    "def evaluate_kfold(X, y, k_values=[3, 5, 10]):\n",
    "    results = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        model = LogisticRegression(max_iter=200)\n",
    "        \n",
    "        # Get scores for each fold\n",
    "        scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "        \n",
    "        results[k] = {\n",
    "            'scores': scores,\n",
    "            'mean': scores.mean(),\n",
    "            'std': scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{k}-Fold Cross-Validation:\")\n",
    "        print(f\"  Scores per fold: {[f'{s:.4f}' for s in scores]}\")\n",
    "        print(f\"  Mean Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate with different K values\n",
    "kfold_results = evaluate_kfold(X, y, k_values=[3, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize K-Fold results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Performance across folds for K=5\n",
    "k = 5\n",
    "fold_scores = kfold_results[k]['scores']\n",
    "fold_numbers = np.arange(1, k + 1)\n",
    "\n",
    "axes[0].plot(fold_numbers, fold_scores, marker='o', linewidth=2, markersize=10, label='Fold Accuracy')\n",
    "axes[0].axhline(y=kfold_results[k]['mean'], color='r', linestyle='--', linewidth=2, label=f'Mean: {kfold_results[k][\"mean\"]:.4f}')\n",
    "axes[0].fill_between(fold_numbers, \n",
    "                      kfold_results[k]['mean'] - kfold_results[k]['std'],\n",
    "                      kfold_results[k]['mean'] + kfold_results[k]['std'],\n",
    "                      alpha=0.2, color='red', label='¬±1 Std Dev')\n",
    "axes[0].set_xlabel('Fold Number', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('5-Fold Cross-Validation: Performance per Fold', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(fold_numbers)\n",
    "axes[0].set_ylim([0.85, 1.0])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Comparison of different K values\n",
    "k_values = list(kfold_results.keys())\n",
    "means = [kfold_results[k]['mean'] for k in k_values]\n",
    "stds = [kfold_results[k]['std'] for k in k_values]\n",
    "\n",
    "axes[1].bar(range(len(k_values)), means, yerr=stds, capsize=5, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_xlabel('K Value', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Accuracy', fontsize=12)\n",
    "axes[1].set_title('Impact of K on Cross-Validation Performance', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(len(k_values)))\n",
    "axes[1].set_xticklabels([f'K={k}' for k in k_values])\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    axes[1].text(i, mean + std + 0.005, f'{mean:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e56dc",
   "metadata": {},
   "source": [
    "### Variance Reduction with K-Fold\n",
    "\n",
    "Notice how K-Fold gives us multiple performance measurements instead of just one. This helps us:\n",
    "\n",
    "1. **Understand stability**: If scores vary wildly across folds, our model might be sensitive to the training data\n",
    "2. **Get confidence intervals**: The standard deviation tells us how reliable our estimate is\n",
    "3. **Detect overfitting**: Large differences between training and validation scores indicate overfitting\n",
    "\n",
    "**Choosing K:**\n",
    "- **Smaller K (3-5)**: Faster, but higher variance in estimates\n",
    "- **Larger K (10)**: More reliable estimates, but computationally expensive\n",
    "- **K = N (Leave-One-Out)**: Maximum data usage, but very slow and high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf12863",
   "metadata": {},
   "source": [
    "## 4. Stratified K-Fold Cross-Validation\n",
    "\n",
    "### Why Stratification Matters\n",
    "\n",
    "Regular K-Fold has a potential problem: **folds might not have balanced class distributions**. This is especially critical when:\n",
    "- Dataset is small\n",
    "- Classes are imbalanced\n",
    "- Some classes are rare\n",
    "\n",
    "**Stratified K-Fold** ensures each fold maintains the same class proportions as the original dataset.\n",
    "\n",
    "**Example:** If your dataset has 60% class A and 40% class B, each fold will also have approximately 60% class A and 40% class B.\n",
    "\n",
    "**When to use:**\n",
    "- Classification problems (almost always)\n",
    "- Imbalanced datasets (absolutely necessary)\n",
    "- Small datasets (highly recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8274e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Compare regular K-Fold vs Stratified K-Fold\n",
    "k = 5\n",
    "\n",
    "# Regular K-Fold\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "regular_scores = cross_val_score(LogisticRegression(max_iter=200), X, y, cv=kfold)\n",
    "\n",
    "# Stratified K-Fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "stratified_scores = cross_val_score(LogisticRegression(max_iter=200), X, y, cv=stratified_kfold)\n",
    "\n",
    "print(\"Regular K-Fold:\")\n",
    "print(f\"  Scores: {[f'{s:.4f}' for s in regular_scores]}\")\n",
    "print(f\"  Mean: {regular_scores.mean():.4f} (+/- {regular_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\nStratified K-Fold:\")\n",
    "print(f\"  Scores: {[f'{s:.4f}' for s in stratified_scores]}\")\n",
    "print(f\"  Mean: {stratified_scores.mean():.4f} (+/- {stratified_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nVariance Reduction: {(regular_scores.std() - stratified_scores.std()) / regular_scores.std() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution in each fold\n",
    "def analyze_fold_distributions(X, y, cv_splitter, cv_name):\n",
    "    fold_distributions = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cv_splitter.split(X, y)):\n",
    "        test_labels = y[test_idx]\n",
    "        class_counts = np.bincount(test_labels, minlength=len(iris.target_names))\n",
    "        class_percentages = class_counts / len(test_labels) * 100\n",
    "        fold_distributions.append(class_percentages)\n",
    "    \n",
    "    return np.array(fold_distributions)\n",
    "\n",
    "# Get distributions\n",
    "regular_dist = analyze_fold_distributions(X, y, kfold, \"Regular\")\n",
    "stratified_dist = analyze_fold_distributions(X, y, stratified_kfold, \"Stratified\")\n",
    "\n",
    "# Expected distribution (from full dataset)\n",
    "expected_dist = np.bincount(y) / len(y) * 100\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "x = np.arange(k)\n",
    "width = 0.25\n",
    "\n",
    "for idx, (dist, title) in enumerate([(regular_dist, 'Regular K-Fold'), \n",
    "                                       (stratified_dist, 'Stratified K-Fold')]):\n",
    "    for i, class_name in enumerate(iris.target_names):\n",
    "        axes[idx].bar(x + i * width, dist[:, i], width, label=class_name, alpha=0.8)\n",
    "        axes[idx].axhline(y=expected_dist[i], color=f'C{i}', linestyle='--', \n",
    "                         linewidth=2, alpha=0.5)\n",
    "    \n",
    "    axes[idx].set_xlabel('Fold Number', fontsize=12)\n",
    "    axes[idx].set_ylabel('Class Percentage (%)', fontsize=12)\n",
    "    axes[idx].set_title(f'{title}: Class Distribution per Fold', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xticks(x + width)\n",
    "    axes[idx].set_xticklabels([f'Fold {i+1}' for i in range(k)])\n",
    "    axes[idx].legend(title='Species')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how Stratified K-Fold maintains class balance across all folds!\")\n",
    "print(\"The dashed lines represent the expected distribution from the full dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f94fa1",
   "metadata": {},
   "source": [
    "## 5. Train-Validation-Test Split\n",
    "\n",
    "### The Three-Way Split\n",
    "\n",
    "When we need to tune hyperparameters, a simple train-test split isn't enough. We need three sets:\n",
    "\n",
    "1. **Training Set (60%)**: Used to train the model\n",
    "2. **Validation Set (20%)**: Used to tune hyperparameters and make model selection decisions\n",
    "3. **Test Set (20%)**: Used ONLY for final evaluation (touched once at the very end)\n",
    "\n",
    "**Why three sets?**\n",
    "- If we tune hyperparameters using the test set, we're indirectly \"training\" on it\n",
    "- This leads to overly optimistic performance estimates\n",
    "- The test set must remain completely unseen until final evaluation\n",
    "\n",
    "**Workflow:**\n",
    "1. Split data into train and temp (temp = validation + test)\n",
    "2. Split temp into validation and test\n",
    "3. Train multiple models with different hyperparameters on training set\n",
    "4. Evaluate each on validation set\n",
    "5. Select best hyperparameters\n",
    "6. Train final model on train + validation\n",
    "7. Evaluate once on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdcb821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three-way split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"  Training: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Total: {len(X)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate hyperparameter tuning workflow\n",
    "print(\"Hyperparameter Tuning: Decision Tree Max Depth\\n\")\n",
    "\n",
    "# Try different max_depth values\n",
    "depths = [2, 3, 4, 5, 6, 8, 10]\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    val_acc = model.score(X_val, y_val)\n",
    "    \n",
    "    train_scores.append(train_acc)\n",
    "    val_scores.append(val_acc)\n",
    "    \n",
    "    print(f\"Depth={depth:2d}: Train={train_acc:.4f}, Validation={val_acc:.4f}, Gap={train_acc-val_acc:.4f}\")\n",
    "\n",
    "# Select best depth based on validation performance\n",
    "best_depth = depths[np.argmax(val_scores)]\n",
    "print(f\"\\nBest max_depth based on validation set: {best_depth}\")\n",
    "\n",
    "# Train final model on train + validation\n",
    "X_train_final = np.vstack([X_train, X_val])\n",
    "y_train_final = np.hstack([y_train, y_val])\n",
    "\n",
    "final_model = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\n",
    "final_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Evaluate on test set (only once!)\n",
    "test_accuracy = final_model.score(X_test, y_test)\n",
    "print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\n‚ö†Ô∏è  Important: We only touch the test set once, at the very end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hyperparameter tuning process\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training vs Validation curves\n",
    "axes[0].plot(depths, train_scores, marker='o', linewidth=2, markersize=8, label='Training', color='#2ecc71')\n",
    "axes[0].plot(depths, val_scores, marker='s', linewidth=2, markersize=8, label='Validation', color='#e74c3c')\n",
    "axes[0].axvline(x=best_depth, color='gold', linestyle='--', linewidth=2, label=f'Best Depth={best_depth}')\n",
    "axes[0].set_xlabel('Max Depth', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Hyperparameter Tuning: Train vs Validation Performance', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0.85, 1.05])\n",
    "\n",
    "# Plot 2: Overfitting gap\n",
    "gaps = np.array(train_scores) - np.array(val_scores)\n",
    "colors = ['green' if gap < 0.05 else 'orange' if gap < 0.1 else 'red' for gap in gaps]\n",
    "\n",
    "bars = axes[1].bar(range(len(depths)), gaps, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_xlabel('Max Depth', fontsize=12)\n",
    "axes[1].set_ylabel('Train - Validation Gap', fontsize=12)\n",
    "axes[1].set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(len(depths)))\n",
    "axes[1].set_xticklabels(depths)\n",
    "axes[1].axhline(y=0.05, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='Acceptable Gap')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, gap) in enumerate(zip(bars, gaps)):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., gap + 0.005, \n",
    "                f'{gap:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0618e04",
   "metadata": {},
   "source": [
    "## 6. Comparison and Best Practices\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "| Method | Best For | Avoid When |\n",
    "|--------|----------|------------|\n",
    "| **Train-Test Split** | Large datasets, quick experiments, baseline models | Small datasets, need robust estimates |\n",
    "| **K-Fold CV** | Model comparison, performance estimation, medium datasets | Very large datasets (slow), time-series data |\n",
    "| **Stratified K-Fold** | Classification (almost always!), imbalanced classes | Regression problems |\n",
    "| **Train-Val-Test** | Hyperparameter tuning, model selection, final evaluation | Simple experiments without tuning |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always use stratification** for classification problems\n",
    "2. **Never tune on test data** - it must remain unseen\n",
    "3. **K-Fold reduces variance** but increases computation time\n",
    "4. **Larger K ‚â† always better** - balance bias-variance and speed\n",
    "5. **For production**: Use K-Fold for model selection, then retrain on all data\n",
    "\n",
    "### The Golden Rule\n",
    "\n",
    "**Test data is sacred** - touch it only once, at the very end, after all decisions are made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Computation time comparison (approximate)\n",
    "methods = ['Train-Test', '5-Fold CV', 'Stratified\\n5-Fold', 'Train-Val-Test']\n",
    "relative_times = [1, 5, 5, 1]\n",
    "colors_time = ['#3498db', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "\n",
    "axes[0, 0].bar(methods, relative_times, color=colors_time, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0, 0].set_ylabel('Relative Computation Time', fontsize=12)\n",
    "axes[0, 0].set_title('Computational Cost Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "for i, time in enumerate(relative_times):\n",
    "    axes[0, 0].text(i, time + 0.2, f'{time}x', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Data usage efficiency\n",
    "data_usage = [80, 100, 100, 60]  # Percentage of data used for training\n",
    "axes[0, 1].bar(methods, data_usage, color=colors_time, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0, 1].set_ylabel('Data Used for Training (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Training Data Utilization', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylim([0, 110])\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "for i, usage in enumerate(data_usage):\n",
    "    axes[0, 1].text(i, usage + 2, f'{usage}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 3: Estimate reliability (inverse of variance)\n",
    "reliability = [3, 9, 10, 5]  # Subjective score\n",
    "axes[1, 0].bar(methods, reliability, color=colors_time, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 0].set_ylabel('Reliability Score', fontsize=12)\n",
    "axes[1, 0].set_title('Performance Estimate Reliability', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylim([0, 11])\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "for i, rel in enumerate(reliability):\n",
    "    axes[1, 0].text(i, rel + 0.3, f'{rel}/10', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 4: Use case suitability matrix\n",
    "use_cases = ['Small\\nDataset', 'Large\\nDataset', 'Imbalanced\\nClasses', 'Hyperparameter\\nTuning']\n",
    "suitability = np.array([\n",
    "    [2, 4, 3, 2],  # Train-Test\n",
    "    [4, 3, 3, 3],  # K-Fold\n",
    "    [5, 4, 5, 4],  # Stratified K-Fold\n",
    "    [3, 4, 4, 5],  # Train-Val-Test\n",
    "])\n",
    "\n",
    "im = axes[1, 1].imshow(suitability.T, cmap='RdYlGn', aspect='auto', vmin=1, vmax=5)\n",
    "axes[1, 1].set_xticks(range(len(methods)))\n",
    "axes[1, 1].set_yticks(range(len(use_cases)))\n",
    "axes[1, 1].set_xticklabels(methods, fontsize=10)\n",
    "axes[1, 1].set_yticklabels(use_cases, fontsize=10)\n",
    "axes[1, 1].set_title('Suitability Matrix (1=Poor, 5=Excellent)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(methods)):\n",
    "    for j in range(len(use_cases)):\n",
    "        text = axes[1, 1].text(i, j, suitability[i, j], ha='center', va='center', \n",
    "                              color='white' if suitability[i, j] > 3 else 'black',\n",
    "                              fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Suitability Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aced117",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned about the four main validation techniques in machine learning:\n",
    "\n",
    "1. **Train-Test Split**: Simple and fast, good for quick experiments\n",
    "2. **K-Fold Cross-Validation**: More reliable estimates, better for model comparison\n",
    "3. **Stratified K-Fold**: Essential for classification, especially with imbalanced data\n",
    "4. **Train-Validation-Test**: Required for hyperparameter tuning and final evaluation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Practice with your own datasets\n",
    "- Try different K values and observe the variance\n",
    "- Always use stratification for classification\n",
    "- Remember: test data is sacred!\n",
    "\n",
    "Happy learning! üéâ"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
