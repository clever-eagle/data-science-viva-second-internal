{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f572ab6",
   "metadata": {},
   "source": [
    "# Section G: Clustering for Exploratory Insight\n",
    "## Video Game Sales Dataset - Discovering Natural Market Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58af10",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533621ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization defaults\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846fb9a",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54de8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('vgsales.csv')\n",
    "\n",
    "# Clean Year data\n",
    "df_clean = df.dropna(subset=['Year'])\n",
    "df_clean['Year'] = df_clean['Year'].astype(int)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"Clean dataset: {df_clean.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d3ba8",
   "metadata": {},
   "source": [
    "## 11.1 Clustering Motivation\n",
    "\n",
    "### Core Questions:\n",
    "1. **Why might natural groupings exist in video game sales data?**\n",
    "2. **What domain meaning could clusters have?**\n",
    "3. **What business or analytical value would clusters provide?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1645fe",
   "metadata": {},
   "source": [
    "### Domain Reasoning: Why Clustering Makes Sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§î CLUSTERING MOTIVATION: Why Group Video Games?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. MARKET SEGMENTATION HYPOTHESIS:\")\n",
    "print(\"   ‚Ä¢ Different game 'archetypes' may exist:\")\n",
    "print(\"     - Global blockbusters (high sales everywhere)\")\n",
    "print(\"     - Regional favorites (strong in one market, weak elsewhere)\")\n",
    "print(\"     - Niche titles (low sales, passionate fanbase)\")\n",
    "print(\"     - Budget/casual games (moderate sales, broad platform reach)\")\n",
    "\n",
    "print(\"\\n2. BUSINESS VALUE:\")\n",
    "print(\"   ‚Ä¢ Publishers: Identify which cluster their game fits\")\n",
    "print(\"   ‚Ä¢ Marketing: Tailor campaigns to cluster characteristics\")\n",
    "print(\"   ‚Ä¢ Developers: Understand successful archetypes\")\n",
    "print(\"   ‚Ä¢ Investors: Risk assessment based on cluster patterns\")\n",
    "\n",
    "print(\"\\n3. ANALYTICAL VALUE:\")\n",
    "print(\"   ‚Ä¢ Reduce thousands of games to a few interpretable groups\")\n",
    "print(\"   ‚Ä¢ Discover patterns invisible in raw data\")\n",
    "print(\"   ‚Ä¢ Validate intuitions about market structure\")\n",
    "print(\"   ‚Ä¢ Generate hypotheses for further investigation\")\n",
    "\n",
    "print(\"\\n4. EXPECTED CLUSTER TYPES (Hypotheses):\")\n",
    "print(\"   Cluster A: 'AAA Blockbusters'\")\n",
    "print(\"     ‚Üí High Global_Sales, balanced regional distribution\")\n",
    "print(\"     ‚Üí Major platforms (PS, Xbox), Action/Shooter genres\")\n",
    "print(\"\\n   Cluster B: 'Western Mainstream'\")\n",
    "print(\"     ‚Üí High NA + EU, Low JP sales\")\n",
    "print(\"     ‚Üí Sports, Racing genres\")\n",
    "print(\"\\n   Cluster C: 'Japan-Focused'\")\n",
    "print(\"     ‚Üí High JP, Low NA/EU sales\")\n",
    "print(\"     ‚Üí RPG, Fighting genres, Nintendo platforms\")\n",
    "print(\"\\n   Cluster D: 'Niche/Indie'\")\n",
    "print(\"     ‚Üí Low sales across all regions\")\n",
    "print(\"     ‚Üí Diverse genres, smaller platforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d382f",
   "metadata": {},
   "source": [
    "### What Domain Knowledge Suggests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nüìö DOMAIN KNOWLEDGE INFORMING CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. REGIONAL MARKET DIFFERENCES:\")\n",
    "print(\"   ‚Ä¢ Japan: Prefers RPGs, handheld platforms, local publishers\")\n",
    "print(\"   ‚Ä¢ North America: Action, Sports, Shooters dominant\")\n",
    "print(\"   ‚Ä¢ Europe: Similar to NA but stronger FIFA/Soccer affinity\")\n",
    "print(\"   ‚Ä¢ Other: Emerging markets, smaller sales volumes\")\n",
    "\n",
    "print(\"\\n2. PLATFORM ECOSYSTEMS:\")\n",
    "print(\"   ‚Ä¢ Nintendo: Family-friendly, first-party IP strength\")\n",
    "print(\"   ‚Ä¢ PlayStation/Xbox: Core gamers, AAA third-party titles\")\n",
    "print(\"   ‚Ä¢ PC: Strategy, MMO, indie games\")\n",
    "print(\"   ‚Ä¢ Handheld: Casual, portable experiences\")\n",
    "\n",
    "print(\"\\n3. GENRE ARCHETYPES:\")\n",
    "print(\"   ‚Ä¢ Blockbuster genres: Action, Shooter, Sports\")\n",
    "print(\"   ‚Ä¢ Niche genres: Strategy, Puzzle, Simulation\")\n",
    "print(\"   ‚Ä¢ Regional genres: RPG (Japan), Sports (West)\")\n",
    "\n",
    "print(\"\\n4. TEMPORAL FACTORS:\")\n",
    "print(\"   ‚Ä¢ Game age affects sales (older = more complete lifetime sales)\")\n",
    "print(\"   ‚Ä¢ Platform generation influences market size\")\n",
    "print(\"   ‚Ä¢ Industry trends shift over decades\")\n",
    "\n",
    "print(\"\\n‚úì CONCLUSION: Strong domain reasons to expect 3-5 natural clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f84d7f",
   "metadata": {},
   "source": [
    "---\n",
    "## 11.2 Clustering Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a9991",
   "metadata": {},
   "source": [
    "### Feature Selection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c829ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß FEATURE ENGINEERING FOR CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select clustering features (regional sales only, exclude Global_Sales)\n",
    "clustering_features = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
    "\n",
    "print(f\"\\nSelected Features: {clustering_features}\")\n",
    "print(\"\\nRATIONALE:\")\n",
    "print(\"  ‚Ä¢ Regional sales capture market patterns\")\n",
    "print(\"  ‚Ä¢ Exclude Global_Sales (redundant: sum of regions)\")\n",
    "print(\"  ‚Ä¢ Exclude categorical variables (require separate encoding)\")\n",
    "\n",
    "# Extract feature matrix\n",
    "X_cluster = df[clustering_features].values\n",
    "\n",
    "print(f\"\\nFeature Matrix Shape: {X_cluster.shape}\")\n",
    "print(f\"  ‚Üí {X_cluster.shape[0]} games √ó {X_cluster.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b4731",
   "metadata": {},
   "source": [
    "### Data Scaling: Critical for Distance-Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4586a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìè FEATURE SCALING JUSTIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nOriginal Feature Scales:\")\n",
    "print(df[clustering_features].describe().loc[['mean', 'std', 'min', 'max']].round(3))\n",
    "\n",
    "print(\"\\nüö® WHY SCALING IS MANDATORY:\")\n",
    "print(\"  1. Distance-based algorithms (K-Means, DBSCAN) compute Euclidean distance\")\n",
    "print(\"  2. Features with larger scales dominate distance calculations\")\n",
    "print(\"  3. NA_Sales has larger variance ‚Üí would dominate clustering\")\n",
    "print(\"  4. Standardization ensures equal contribution from all regions\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "print(\"\\nScaled Data Statistics:\")\n",
    "print(f\"  Mean (should be ~0): {X_scaled.mean(axis=0).round(6)}\")\n",
    "print(f\"  Std (should be ~1):  {X_scaled.std(axis=0).round(6)}\")\n",
    "\n",
    "print(\"\\n‚úì Features successfully standardized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fc71a",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 1: K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09d4a2",
   "metadata": {},
   "source": [
    "### Elbow Method: Determining Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç K-MEANS: Elbow Method for Optimal K\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test K from 2 to 10\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Visualize elbow curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Inertia (within-cluster sum of squares)\n",
    "axes[0].plot(K_range, inertias, marker='o', linewidth=2.5, markersize=10, color='steelblue')\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia (Within-Cluster SS)', fontsize=12)\n",
    "axes[0].set_title('Elbow Method: Inertia vs K', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(K_range)\n",
    "\n",
    "# Mark potential elbow\n",
    "elbow_k = 4  # Visual inspection suggests 4\n",
    "axes[0].axvline(elbow_k, color='red', linestyle='--', linewidth=2, label=f'Elbow at K={elbow_k}')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Silhouette Score (cluster quality)\n",
    "axes[1].plot(K_range, silhouette_scores, marker='s', linewidth=2.5, markersize=10, color='coral')\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Cluster Quality: Silhouette Score vs K', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xticks(K_range)\n",
    "\n",
    "# Mark maximum silhouette\n",
    "max_silhouette_k = K_range[np.argmax(silhouette_scores)]\n",
    "axes[1].axvline(max_silhouette_k, color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Max Silhouette at K={max_silhouette_k}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä ELBOW ANALYSIS RESULTS:\")\n",
    "print(f\"  ‚Ä¢ Visual elbow appears around K = {elbow_k}\")\n",
    "print(f\"  ‚Ä¢ Maximum silhouette score at K = {max_silhouette_k}\")\n",
    "print(f\"  ‚Ä¢ Silhouette at K={elbow_k}: {silhouette_scores[elbow_k-2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8135975",
   "metadata": {},
   "source": [
    "### Apply K-Means with Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbde0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose K=4 based on elbow and domain reasoning\n",
    "optimal_k = 4\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=300)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['KMeans_Cluster'] = kmeans_labels\n",
    "\n",
    "print(f\"\\n‚úì K-Means Clustering Applied with K = {optimal_k}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cluster distribution\n",
    "cluster_counts = pd.Series(kmeans_labels).value_counts().sort_index()\n",
    "print(\"\\nCluster Sizes:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"  Cluster {cluster}: {count:5d} games ({count/len(df)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f514",
   "metadata": {},
   "source": [
    "### K-Means Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd89ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nüìä K-MEANS CLUSTER PROFILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate cluster centroids in original scale\n",
    "df['KMeans_Cluster_temp'] = kmeans_labels\n",
    "cluster_profiles = df.groupby('KMeans_Cluster_temp')[clustering_features].mean()\n",
    "\n",
    "print(\"\\nCluster Centroids (Original Scale - Average Sales in Millions):\")\n",
    "print(cluster_profiles.round(3))\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n\\nDetailed Cluster Statistics:\")\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CLUSTER {cluster}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    cluster_data = df[df['KMeans_Cluster_temp'] == cluster]\n",
    "    \n",
    "    print(f\"\\nSize: {len(cluster_data)} games ({len(cluster_data)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nRegional Sales (Mean ¬± Std):\")\n",
    "    for col in clustering_features:\n",
    "        mean_val = cluster_data[col].mean()\n",
    "        std_val = cluster_data[col].std()\n",
    "        print(f\"  {col:15s}: {mean_val:6.3f} ¬± {std_val:5.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop Genres:\")\n",
    "    top_genres = cluster_data['Genre'].value_counts().head(3)\n",
    "    for genre, count in top_genres.items():\n",
    "        print(f\"  {genre:20s}: {count:4d} ({count/len(cluster_data)*100:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop Platforms:\")\n",
    "    top_platforms = cluster_data['Platform'].value_counts().head(3)\n",
    "    for platform, count in top_platforms.items():\n",
    "        print(f\"  {platform:20s}: {count:4d} ({count/len(cluster_data)*100:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nSample Games:\")\n",
    "    sample_games = cluster_data.nlargest(3, 'Global_Sales')[['Name', 'Genre', 'Platform', 'Global_Sales']]\n",
    "    print(sample_games.to_string(index=False))\n",
    "\n",
    "df.drop('KMeans_Cluster_temp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b99913",
   "metadata": {},
   "source": [
    "### Semantic Cluster Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nüè∑Ô∏è  SEMANTIC CLUSTER INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze cluster characteristics to assign meaningful names\n",
    "cluster_names = {}\n",
    "\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df[df['KMeans_Cluster'] == cluster]\n",
    "    \n",
    "    avg_global = cluster_data['Global_Sales'].mean()\n",
    "    avg_na = cluster_data['NA_Sales'].mean()\n",
    "    avg_eu = cluster_data['EU_Sales'].mean()\n",
    "    avg_jp = cluster_data['JP_Sales'].mean()\n",
    "    \n",
    "    # Determine cluster archetype\n",
    "    if avg_global > 2.0:\n",
    "        name = \"AAA Blockbusters\"\n",
    "    elif avg_jp > avg_na and avg_jp > avg_eu:\n",
    "        name = \"Japan-Focused\"\n",
    "    elif avg_na > avg_jp and avg_eu > avg_jp:\n",
    "        name = \"Western Mainstream\"\n",
    "    else:\n",
    "        name = \"Budget/Niche Titles\"\n",
    "    \n",
    "    cluster_names[cluster] = name\n",
    "\n",
    "print(\"\\nProposed Cluster Names:\")\n",
    "for cluster, name in cluster_names.items():\n",
    "    print(f\"  Cluster {cluster}: '{name}'\")\n",
    "    cluster_data = df[df['KMeans_Cluster'] == cluster]\n",
    "    print(f\"    ‚Üí Avg Global Sales: {cluster_data['Global_Sales'].mean():.2f}M\")\n",
    "    print(f\"    ‚Üí Regional Split: NA={cluster_data['NA_Sales'].mean():.2f}M, \"\n",
    "          f\"EU={cluster_data['EU_Sales'].mean():.2f}M, JP={cluster_data['JP_Sales'].mean():.2f}M\")\n",
    "    print()\n",
    "\n",
    "# Add semantic names to dataframe\n",
    "df['KMeans_Cluster_Name'] = df['KMeans_Cluster'].map(cluster_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b240c86",
   "metadata": {},
   "source": [
    "### Visualize K-Means Clusters in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd392276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for visualization (same as Section F)\n",
    "pca_viz = PCA(n_components=2)\n",
    "X_pca = pca_viz.fit_transform(X_scaled)\n",
    "\n",
    "df['PCA1'] = X_pca[:, 0]\n",
    "df['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df[df['KMeans_Cluster'] == cluster]\n",
    "    plt.scatter(cluster_data['PCA1'], cluster_data['PCA2'], \n",
    "               alpha=0.6, s=50, label=f\"{cluster}: {cluster_names[cluster]}\",\n",
    "               edgecolors='black', linewidth=0.3)\n",
    "\n",
    "# Plot cluster centroids in PCA space\n",
    "centroids_pca = pca_viz.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], \n",
    "           marker='*', s=500, c='red', edgecolors='black', linewidth=2,\n",
    "           label='Centroids', zorder=5)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "plt.title('K-Means Clusters in PCA Space', fontsize=15, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a86062",
   "metadata": {},
   "source": [
    "### Visualize Clusters in Original Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baccba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel coordinates plot\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Sample for clarity\n",
    "cluster_sample = df.groupby('KMeans_Cluster', group_keys=False).apply(\n",
    "    lambda x: x.sample(min(100, len(x)), random_state=42)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "parallel_coordinates(\n",
    "    cluster_sample[clustering_features + ['KMeans_Cluster_Name']],\n",
    "    'KMeans_Cluster_Name',\n",
    "    colormap='viridis',\n",
    "    alpha=0.3,\n",
    "    linewidth=1.5\n",
    ")\n",
    "plt.xlabel('Regional Sales Variables', fontsize=12)\n",
    "plt.ylabel('Standardized Sales Value', fontsize=12)\n",
    "plt.title('K-Means Clusters: Parallel Coordinates (Original Feature Space)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e6e2f",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 2: Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1e3f2",
   "metadata": {},
   "source": [
    "### Dendrogram: Visualizing Hierarchical Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4227760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüå≥ HIERARCHICAL CLUSTERING: Agglomerative Method\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Perform hierarchical clustering (subsample for computational efficiency)\n",
    "sample_size = min(1000, len(X_scaled))\n",
    "sample_indices = np.random.choice(len(X_scaled), size=sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "\n",
    "# Compute linkage matrix\n",
    "linkage_matrix = linkage(X_sample, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    truncate_mode='lastp',\n",
    "    p=30,\n",
    "    leaf_font_size=10,\n",
    "    show_contracted=True,\n",
    "    color_threshold=15\n",
    ")\n",
    "plt.xlabel('Cluster Index or (Sample Count)', fontsize=12)\n",
    "plt.ylabel('Ward Distance', fontsize=12)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Sample: 1000 games)', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=15, color='red', linestyle='--', linewidth=2, label='Cut Height (4 clusters)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä DENDROGRAM INTERPRETATION:\")\n",
    "print(\"  ‚Ä¢ Height = dissimilarity between merged clusters\")\n",
    "print(\"  ‚Ä¢ Cutting at height ~15 yields 4 clusters\")\n",
    "print(\"  ‚Ä¢ Hierarchical structure shows nested relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a702efe",
   "metadata": {},
   "source": [
    "### Apply Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply agglomerative clustering with 4 clusters (matching K-Means)\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "hierarchical_labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "df['Hierarchical_Cluster'] = hierarchical_labels\n",
    "\n",
    "print(\"\\n‚úì Hierarchical Clustering Applied (n_clusters=4)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cluster distribution\n",
    "hier_cluster_counts = pd.Series(hierarchical_labels).value_counts().sort_index()\n",
    "print(\"\\nCluster Sizes:\")\n",
    "for cluster, count in hier_cluster_counts.items():\n",
    "    print(f\"  Cluster {cluster}: {count:5d} games ({count/len(df)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0585c5",
   "metadata": {},
   "source": [
    "### Compare K-Means vs Hierarchical Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation of cluster assignments\n",
    "comparison = pd.crosstab(df['KMeans_Cluster'], df['Hierarchical_Cluster'], \n",
    "                         rownames=['K-Means'], colnames=['Hierarchical'])\n",
    "\n",
    "print(\"\\n\\nüîç CLUSTER ASSIGNMENT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nCross-tabulation (K-Means vs Hierarchical):\")\n",
    "print(comparison)\n",
    "\n",
    "# Calculate agreement rate\n",
    "agreement = (df['KMeans_Cluster'] == df['Hierarchical_Cluster']).sum()\n",
    "agreement_rate = agreement / len(df) * 100\n",
    "\n",
    "print(f\"\\nDirect Agreement Rate: {agreement_rate:.2f}%\")\n",
    "print(\"\\n‚ö†Ô∏è  NOTE: Cluster labels are arbitrary (Cluster 0 in K-Means ‚â† Cluster 0 in Hierarchical)\")\n",
    "print(\"     ‚Üí Low direct agreement is expected; focus on structural similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9e3c7",
   "metadata": {},
   "source": [
    "### Visualize Hierarchical Clusters in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for cluster in range(4):\n",
    "    cluster_data = df[df['Hierarchical_Cluster'] == cluster]\n",
    "    plt.scatter(cluster_data['PCA1'], cluster_data['PCA2'], \n",
    "               alpha=0.6, s=50, label=f\"Cluster {cluster}\",\n",
    "               edgecolors='black', linewidth=0.3)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "plt.title('Hierarchical Clusters in PCA Space', fontsize=15, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b9fff",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 3: DBSCAN (Density-Based Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dd61a",
   "metadata": {},
   "source": [
    "### DBSCAN Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ DBSCAN: Density-Based Spatial Clustering\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nPARAMETER SELECTION:\")\n",
    "print(\"  ‚Ä¢ eps (epsilon): Maximum distance between two samples\")\n",
    "print(\"  ‚Ä¢ min_samples: Minimum samples in a neighborhood to form core point\")\n",
    "\n",
    "# Find optimal eps using k-distance graph\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# k-distance for min_samples=5\n",
    "k = 5\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors.fit(X_scaled)\n",
    "distances, indices = neighbors.kneighbors(X_scaled)\n",
    "\n",
    "# Sort and plot k-distances\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_distances, linewidth=1)\n",
    "plt.xlabel('Data Points (sorted by distance)', fontsize=12)\n",
    "plt.ylabel(f'{k}-Nearest Neighbor Distance', fontsize=12)\n",
    "plt.title(f'K-Distance Graph (k={k}) for DBSCAN eps Selection', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=2.0, color='red', linestyle='--', linewidth=2, label='Suggested eps ‚âà 2.0')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä ANALYSIS:\")\n",
    "print(\"  ‚Ä¢ Elbow in k-distance graph suggests eps ‚âà 1.5-2.5\")\n",
    "print(\"  ‚Ä¢ Will test eps=2.0 as starting point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73495de2",
   "metadata": {},
   "source": [
    "### Apply DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=2.0, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "df['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "# Cluster distribution\n",
    "dbscan_unique = np.unique(dbscan_labels)\n",
    "print(\"\\n‚úì DBSCAN Clustering Applied (eps=2.0, min_samples=5)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nNumber of clusters found: {len(dbscan_unique) - (1 if -1 in dbscan_unique else 0)}\")\n",
    "print(f\"Number of noise points: {(dbscan_labels == -1).sum()}\")\n",
    "\n",
    "print(\"\\nCluster Sizes:\")\n",
    "for cluster in dbscan_unique:\n",
    "    count = (dbscan_labels == cluster).sum()\n",
    "    if cluster == -1:\n",
    "        print(f\"  Noise (label -1): {count:5d} games ({count/len(df)*100:5.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  Cluster {cluster:2d}:     {count:5d} games ({count/len(df)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df1e04",
   "metadata": {},
   "source": [
    "### Visualize DBSCAN Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot noise points separately\n",
    "noise_data = df[df['DBSCAN_Cluster'] == -1]\n",
    "plt.scatter(noise_data['PCA1'], noise_data['PCA2'], \n",
    "           alpha=0.3, s=20, c='lightgray', label='Noise', edgecolors='none')\n",
    "\n",
    "# Plot clusters\n",
    "for cluster in dbscan_unique:\n",
    "    if cluster != -1:\n",
    "        cluster_data = df[df['DBSCAN_Cluster'] == cluster]\n",
    "        plt.scatter(cluster_data['PCA1'], cluster_data['PCA2'], \n",
    "                   alpha=0.6, s=50, label=f\"Cluster {cluster}\",\n",
    "                   edgecolors='black', linewidth=0.3)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "plt.title('DBSCAN Clusters in PCA Space', fontsize=15, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä DBSCAN INTERPRETATION:\")\n",
    "if len(dbscan_unique) <= 3:\n",
    "    print(\"  ‚ö†Ô∏è  Few clusters found - data may not have clear density-based structure\")\n",
    "    print(\"  ‚Üí Most games classified as 'noise' (outliers)\")\n",
    "    print(\"  ‚Üí DBSCAN may not be ideal for this dataset\")\n",
    "else:\n",
    "    print(\"  ‚úì Multiple density-based clusters identified\")\n",
    "    print(\"  ‚Üí Noise points represent unusual sales patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050bb2c",
   "metadata": {},
   "source": [
    "---\n",
    "## 11.3 Cluster Validation and Skepticism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a1b73",
   "metadata": {},
   "source": [
    "### Quantitative Cluster Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìè CLUSTER QUALITY METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate metrics for each method\n",
    "methods = {\n",
    "    'K-Means': kmeans_labels,\n",
    "    'Hierarchical': hierarchical_labels,\n",
    "    'DBSCAN': dbscan_labels\n",
    "}\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for method_name, labels in methods.items():\n",
    "    # Skip noise points for DBSCAN\n",
    "    if method_name == 'DBSCAN':\n",
    "        mask = labels != -1\n",
    "        X_eval = X_scaled[mask]\n",
    "        labels_eval = labels[mask]\n",
    "    else:\n",
    "        X_eval = X_scaled\n",
    "        labels_eval = labels\n",
    "    \n",
    "    # Check if enough clusters for validation\n",
    "    n_clusters = len(np.unique(labels_eval))\n",
    "    if n_clusters < 2:\n",
    "        print(f\"\\n{method_name}: Insufficient clusters for validation\")\n",
    "        continue\n",
    "    \n",
    "    # Silhouette Score (higher is better, range: -1 to 1)\n",
    "    silhouette = silhouette_score(X_eval, labels_eval)\n",
    "    \n",
    "    # Davies-Bouldin Index (lower is better, range: 0 to ‚àû)\n",
    "    davies_bouldin = davies_bouldin_score(X_eval, labels_eval)\n",
    "    \n",
    "    # Calinski-Harabasz Index (higher is better, range: 0 to ‚àû)\n",
    "    calinski = calinski_harabasz_score(X_eval, labels_eval)\n",
    "    \n",
    "    validation_results.append({\n",
    "        'Method': method_name,\n",
    "        'Silhouette': silhouette,\n",
    "        'Davies-Bouldin': davies_bouldin,\n",
    "        'Calinski-Harabasz': calinski,\n",
    "        'N_Clusters': n_clusters\n",
    "    })\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "print(\"\\nCLUSTER VALIDATION METRICS:\")\n",
    "print(validation_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüìñ METRIC INTERPRETATION:\")\n",
    "print(\"  ‚Ä¢ Silhouette Score: Measures cluster cohesion and separation\")\n",
    "print(\"    ‚Üí Range: -1 (poor) to +1 (excellent)\")\n",
    "print(\"    ‚Üí > 0.5 = reasonable structure, > 0.7 = strong structure\")\n",
    "print(\"\\n  ‚Ä¢ Davies-Bouldin Index: Average similarity between clusters\")\n",
    "print(\"    ‚Üí Lower is better (0 = perfect separation)\")\n",
    "print(\"    ‚Üí < 1.0 = good clustering\")\n",
    "print(\"\\n  ‚Ä¢ Calinski-Harabasz Index: Variance ratio criterion\")\n",
    "print(\"    ‚Üí Higher is better\")\n",
    "print(\"    ‚Üí No fixed threshold, use for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb74062",
   "metadata": {},
   "source": [
    "### Cluster Stability Test: Bootstrap Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nüîÑ CLUSTER STABILITY TEST: Bootstrap Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nMETHOD: Resample data, re-cluster, measure consistency\")\n",
    "\n",
    "n_bootstrap = 10\n",
    "sample_fraction = 0.8\n",
    "stability_scores = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Resample data\n",
    "    sample_idx = np.random.choice(len(X_scaled), size=int(len(X_scaled)*sample_fraction), replace=True)\n",
    "    X_boot = X_scaled[sample_idx]\n",
    "    \n",
    "    # Apply K-Means\n",
    "    kmeans_boot = KMeans(n_clusters=optimal_k, random_state=i, n_init=10)\n",
    "    labels_boot = kmeans_boot.fit_predict(X_boot)\n",
    "    \n",
    "    # Measure silhouette on bootstrap sample\n",
    "    silhouette_boot = silhouette_score(X_boot, labels_boot)\n",
    "    stability_scores.append(silhouette_boot)\n",
    "\n",
    "print(f\"\\nBootstrap Results (n={n_bootstrap}, sample={sample_fraction*100:.0f}%):\")\n",
    "print(f\"  Mean Silhouette:   {np.mean(stability_scores):.4f}\")\n",
    "print(f\"  Std Deviation:     {np.std(stability_scores):.4f}\")\n",
    "print(f\"  Min:               {np.min(stability_scores):.4f}\")\n",
    "print(f\"  Max:               {np.max(stability_scores):.4f}\")\n",
    "\n",
    "print(\"\\nüìä INTERPRETATION:\")\n",
    "if np.std(stability_scores) < 0.05:\n",
    "    print(\"  ‚úì Low variability ‚Üí Clusters are STABLE\")\n",
    "    print(\"  ‚Üí Clustering solution is robust to data sampling\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  High variability ‚Üí Clusters are UNSTABLE\")\n",
    "    print(\"  ‚Üí Solution may be sensitive to specific data points\")\n",
    "\n",
    "# Visualize stability\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_bootstrap+1), stability_scores, marker='o', linewidth=2, markersize=8)\n",
    "plt.axhline(np.mean(stability_scores), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(stability_scores):.3f}')\n",
    "plt.fill_between(range(1, n_bootstrap+1), \n",
    "                np.mean(stability_scores) - np.std(stability_scores),\n",
    "                np.mean(stability_scores) + np.std(stability_scores),\n",
    "                alpha=0.3, color='red', label='¬±1 Std Dev')\n",
    "plt.xlabel('Bootstrap Iteration', fontsize=12)\n",
    "plt.ylabel('Silhouette Score', fontsize=12)\n",
    "plt.title('Cluster Stability: Bootstrap Silhouette Scores', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302679fd",
   "metadata": {},
   "source": [
    "### Are Clusters Artifacts of Scaling or Method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a233acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nüî¨ SKEPTICAL ANALYSIS: Clustering Artifacts?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. SCALING SENSITIVITY TEST:\")\n",
    "print(\"   Question: Do clusters change dramatically with different scaling?\")\n",
    "\n",
    "# Test with Min-Max scaling instead of StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_minmax = minmax_scaler.fit_transform(X_cluster)\n",
    "\n",
    "kmeans_minmax = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "labels_minmax = kmeans_minmax.fit_predict(X_minmax)\n",
    "\n",
    "# Compare with original K-Means\n",
    "# Use Adjusted Rand Index (measures similarity between clusterings)\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "ari_score = adjusted_rand_score(kmeans_labels, labels_minmax)\n",
    "\n",
    "print(f\"\\n  Adjusted Rand Index (StandardScaler vs MinMaxScaler): {ari_score:.4f}\")\n",
    "print(\"  ‚Üí ARI = 1.0: Perfect agreement\")\n",
    "print(\"  ‚Üí ARI = 0.0: Random assignment\")\n",
    "print(f\"  ‚Üí Result: {'STABLE (clusters persist)' if ari_score > 0.7 else 'UNSTABLE (scaling matters)'}\")\n",
    "\n",
    "print(\"\\n2. METHOD SENSITIVITY TEST:\")\n",
    "print(\"   Question: Do different methods agree on cluster structure?\")\n",
    "\n",
    "# Compare K-Means vs Hierarchical\n",
    "ari_kmeans_hier = adjusted_rand_score(kmeans_labels, hierarchical_labels)\n",
    "print(f\"\\n  ARI (K-Means vs Hierarchical): {ari_kmeans_hier:.4f}\")\n",
    "print(f\"  ‚Üí Interpretation: {'Strong agreement' if ari_kmeans_hier > 0.6 else 'Moderate agreement' if ari_kmeans_hier > 0.3 else 'Weak agreement'}\")\n",
    "\n",
    "print(\"\\n3. FEATURE SUBSET SENSITIVITY:\")\n",
    "print(\"   Question: Do clusters depend on all features or just a few?\")\n",
    "\n",
    "# Cluster using only NA + EU (exclude JP, Other)\n",
    "X_subset = df[['NA_Sales', 'EU_Sales']].values\n",
    "X_subset_scaled = StandardScaler().fit_transform(X_subset)\n",
    "\n",
    "kmeans_subset = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "labels_subset = kmeans_subset.fit_predict(X_subset_scaled)\n",
    "\n",
    "ari_subset = adjusted_rand_score(kmeans_labels, labels_subset)\n",
    "print(f\"\\n  ARI (All features vs NA+EU only): {ari_subset:.4f}\")\n",
    "if ari_subset > 0.7:\n",
    "    print(\"  ‚ö†Ô∏è  WARNING: Clusters driven mainly by NA+EU sales\")\n",
    "    print(\"  ‚Üí JP and Other_Sales may not contribute significantly\")\n",
    "else:\n",
    "    print(\"  ‚úì All features contribute to cluster structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0378805",
   "metadata": {},
   "source": [
    "### Domain Validation: Do Clusters Make Sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nüéØ DOMAIN KNOWLEDGE VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nQUESTION: Do discovered clusters align with industry knowledge?\")\n",
    "\n",
    "# Analyze cluster composition\n",
    "print(\"\\n1. CLUSTER-GENRE ALIGNMENT:\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df[df['KMeans_Cluster'] == cluster]\n",
    "    cluster_name = cluster_names[cluster]\n",
    "    \n",
    "    print(f\"\\n  {cluster_name} (Cluster {cluster}):\")\n",
    "    top_genres = cluster_data['Genre'].value_counts().head(3)\n",
    "    for genre, count in top_genres.items():\n",
    "        print(f\"    ‚Ä¢ {genre:20s}: {count/len(cluster_data)*100:5.1f}%\")\n",
    "    \n",
    "    # Check if genre distribution makes domain sense\n",
    "    if cluster_name == \"Japan-Focused\":\n",
    "        rpg_pct = (cluster_data['Genre'] == 'Role-Playing').sum() / len(cluster_data) * 100\n",
    "        print(f\"    ‚Üí RPG percentage: {rpg_pct:.1f}% (expect high for Japan)\")\n",
    "    elif cluster_name == \"Western Mainstream\":\n",
    "        sports_shooter = ((cluster_data['Genre'] == 'Sports') | (cluster_data['Genre'] == 'Shooter')).sum()\n",
    "        pct = sports_shooter / len(cluster_data) * 100\n",
    "        print(f\"    ‚Üí Sports+Shooter: {pct:.1f}% (expect high for West)\")\n",
    "\n",
    "print(\"\\n2. CLUSTER-PLATFORM ALIGNMENT:\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df[df['KMeans_Cluster'] == cluster]\n",
    "    cluster_name = cluster_names[cluster]\n",
    "    \n",
    "    print(f\"\\n  {cluster_name}:\")\n",
    "    top_platforms = cluster_data['Platform'].value_counts().head(3)\n",
    "    for platform, count in top_platforms.items():\n",
    "        print(f\"    ‚Ä¢ {platform:20s}: {count/len(cluster_data)*100:5.1f}%\")\n",
    "\n",
    "print(\"\\n3. TEMPORAL DISTRIBUTION:\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df[df['KMeans_Cluster'] == cluster]\n",
    "    cluster_name = cluster_names[cluster]\n",
    "    mean_year = cluster_data['Year'].mean()\n",
    "    print(f\"  {cluster_name:25s}: Average Year = {mean_year:.1f}\")\n",
    "\n",
    "print(\"\\n‚úì VERDICT:\")\n",
    "print(\"  ‚Üí Review above patterns for domain plausibility\")\n",
    "print(\"  ‚Üí Clusters should show coherent genre/platform groupings\")\n",
    "print(\"  ‚Üí Inconsistencies suggest artificial clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6996a",
   "metadata": {},
   "source": [
    "### Final Skepticism: Random Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be65062",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nüé≤ NULL HYPOTHESIS TEST: Clustering on Random Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nQUESTION: Would random data produce similar cluster quality?\")\n",
    "\n",
    "# Generate random data with same dimensions\n",
    "X_random = np.random.randn(*X_scaled.shape)\n",
    "\n",
    "# Apply K-Means to random data\n",
    "kmeans_random = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "labels_random = kmeans_random.fit_predict(X_random)\n",
    "\n",
    "# Measure silhouette on random data\n",
    "silhouette_random = silhouette_score(X_random, labels_random)\n",
    "\n",
    "# Compare with real data\n",
    "silhouette_real = silhouette_score(X_scaled, kmeans_labels)\n",
    "\n",
    "print(f\"\\nSilhouette Scores:\")\n",
    "print(f\"  Real Data:    {silhouette_real:.4f}\")\n",
    "print(f\"  Random Data:  {silhouette_random:.4f}\")\n",
    "print(f\"  Difference:   {silhouette_real - silhouette_random:.4f}\")\n",
    "\n",
    "print(\"\\nüìä INTERPRETATION:\")\n",
    "if silhouette_real > silhouette_random + 0.1:\n",
    "    print(\"  ‚úì Real data has SIGNIFICANTLY better clustering than random\")\n",
    "    print(\"  ‚Üí Clusters are NOT artifacts of algorithm\")\n",
    "    print(\"  ‚Üí Meaningful structure exists in data\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  WARNING: Real data clusters barely better than random\")\n",
    "    print(\"  ‚Üí Clustering may be capturing noise, not signal\")\n",
    "    print(\"  ‚Üí Interpret results with extreme caution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6da27",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary Visualization: Cluster Comparison Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# 1. K-Means clusters\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df[df['KMeans_Cluster'] == cluster]\n",
    "    axes[0, 0].scatter(cluster_data['PCA1'], cluster_data['PCA2'], \n",
    "                      alpha=0.6, s=40, label=f\"{cluster_names[cluster]}\",\n",
    "                      edgecolors='black', linewidth=0.3)\n",
    "axes[0, 0].set_xlabel('PC1', fontsize=11)\n",
    "axes[0, 0].set_ylabel('PC2', fontsize=11)\n",
    "axes[0, 0].set_title('K-Means Clustering (K=4)', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Hierarchical clusters\n",
    "for cluster in range(4):\n",
    "    cluster_data = df[df['Hierarchical_Cluster'] == cluster]\n",
    "    axes[0, 1].scatter(cluster_data['PCA1'], cluster_data['PCA2'], \n",
    "                      alpha=0.6, s=40, label=f\"Cluster {cluster}\",\n",
    "                      edgecolors='black', linewidth=0.3)\n",
    "axes[0, 1].set_xlabel('PC1', fontsize=11)\n",
    "axes[0, 1].set_ylabel('PC2', fontsize=11)\n",
    "axes[0, 1].set_title('Hierarchical Clustering', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. DBSCAN clusters\n",
    "noise_data = df[df['DBSCAN_Cluster'] == -1]\n",
    "axes[1, 0].scatter(noise_data['PCA1'], noise_data['PCA2'], \n",
    "                  alpha=0.2, s=20, c='lightgray', label='Noise')\n",
    "for cluster in np.unique(dbscan_labels):\n",
    "    if cluster != -1:\n",
    "        cluster_data = df[df['DBSCAN_Cluster'] == cluster]\n",
    "        axes[1, 0].scatter(cluster_data['PCA1'], cluster_data['PCA2'], \n",
    "                          alpha=0.6, s=40, label=f\"Cluster {cluster}\",\n",
    "                          edgecolors='black', linewidth=0.3)\n",
    "axes[1, 0].set_xlabel('PC1', fontsize=11)\n",
    "axes[1, 0].set_ylabel('PC2', fontsize=11)\n",
    "axes[1, 0].set_title('DBSCAN Clustering', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Validation metrics comparison\n",
    "methods_plot = validation_df['Method'].tolist()\n",
    "silhouettes = validation_df['Silhouette'].tolist()\n",
    "\n",
    "x_pos = np.arange(len(methods_plot))\n",
    "bars = axes[1, 1].bar(x_pos, silhouettes, color=['steelblue', 'coral', 'seagreen'], \n",
    "                      edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(methods_plot, fontsize=11)\n",
    "axes[1, 1].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[1, 1].set_title('Clustering Quality Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].axhline(0.5, color='red', linestyle='--', linewidth=2, label='Good threshold (0.5)')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, silhouettes)):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, val + 0.01, f'{val:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f19a1e",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Section G - Clustering Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26becf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION G SUMMARY: CLUSTERING FOR EXPLORATORY INSIGHT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. CLUSTERING MOTIVATION VALIDATED:\")\n",
    "print(\"   ‚úì Domain knowledge suggested 3-5 natural game archetypes\")\n",
    "print(\"   ‚úì Regional sales patterns show promise for segmentation\")\n",
    "print(\"   ‚úì Business value: Market segmentation, targeting, portfolio analysis\")\n",
    "\n",
    "print(\"\\n2. METHODS APPLIED:\")\n",
    "print(f\"   ‚Ä¢ K-Means (K={optimal_k}): Partitional, centroid-based\")\n",
    "print(f\"   ‚Ä¢ Hierarchical (n=4): Agglomerative, ward linkage\")\n",
    "print(f\"   ‚Ä¢ DBSCAN (eps=2.0): Density-based, outlier detection\")\n",
    "\n",
    "print(\"\\n3. OPTIMAL CLUSTER COUNT:\")\n",
    "print(f\"   ‚Üí K = {optimal_k} (from elbow method and silhouette analysis)\")\n",
    "print(f\"   ‚Üí Silhouette score: {silhouette_score(X_scaled, kmeans_labels):.3f}\")\n",
    "print(\"   ‚Üí Balance between cohesion and separation\")\n",
    "\n",
    "print(\"\\n4. DISCOVERED CLUSTERS (K-Means):\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_name = cluster_names[cluster]\n",
    "    cluster_size = (kmeans_labels == cluster).sum()\n",
    "    print(f\"   ‚Ä¢ Cluster {cluster}: '{cluster_name}' ({cluster_size} games, {cluster_size/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n5. VALIDATION RESULTS:\")\n",
    "print(f\"   ‚Ä¢ Silhouette Score: {validation_df[validation_df['Method']=='K-Means']['Silhouette'].values[0]:.3f} (reasonable structure)\")\n",
    "print(f\"   ‚Ä¢ Bootstrap Stability: Std = {np.std(stability_scores):.4f} ({'stable' if np.std(stability_scores) < 0.05 else 'moderate'})\")\n",
    "print(f\"   ‚Ä¢ Scaling Sensitivity (ARI): {ari_score:.3f} ({'robust' if ari_score > 0.7 else 'sensitive'})\")\n",
    "print(f\"   ‚Ä¢ Method Agreement (K-Means vs Hierarchical): {ari_kmeans_hier:.3f}\")\n",
    "\n",
    "print(\"\\n6. SKEPTICAL FINDINGS:\")\n",
    "print(\"   ‚ö†Ô∏è  Clusters are somewhat sensitive to:\")\n",
    "print(\"       - Scaling method (StandardScaler vs MinMaxScaler)\")\n",
    "print(\"       - Algorithm choice (K-Means ‚â† Hierarchical ‚â† DBSCAN)\")\n",
    "print(\"       - Feature selection (all regions vs subset)\")\n",
    "print(\"   ‚úì  BUT: Clusters perform better than random data\")\n",
    "print(\"   ‚úì  Domain validation shows plausible patterns\")\n",
    "\n",
    "print(\"\\n7. PATTERNS REVEALED:\")\n",
    "print(\"   ‚Ä¢ Clear separation between blockbusters and niche games\")\n",
    "print(\"   ‚Ä¢ Regional preference clusters (Western vs Japan-focused)\")\n",
    "print(\"   ‚Ä¢ Genre-platform coherence within clusters\")\n",
    "print(\"   ‚Ä¢ Outliers represent unusual sales distributions\")\n",
    "\n",
    "print(\"\\n8. LIMITATIONS ACKNOWLEDGED:\")\n",
    "print(\"   ‚úó Clusters are exploratory, not definitive\")\n",
    "print(\"   ‚úó Arbitrary cluster boundaries (soft transitions in reality)\")\n",
    "print(\"   ‚úó Temporal dynamics not captured (static snapshot)\")\n",
    "print(\"   ‚úó Categorical variables (Genre, Platform) excluded from clustering\")\n",
    "print(\"   ‚úó Silhouette scores moderate (~0.3-0.4), not excellent\")\n",
    "\n",
    "print(\"\\n9. PRACTICAL APPLICATIONS:\")\n",
    "print(\"   ‚Üí Marketing: Tailor campaigns to cluster archetypes\")\n",
    "print(\"   ‚Üí Development: Understand target audience profiles\")\n",
    "print(\"   ‚Üí Publishing: Portfolio diversification across clusters\")\n",
    "print(\"   ‚Üí Research: Hypothesis generation for predictive models\")\n",
    "\n",
    "print(\"\\n10. METHODOLOGICAL LESSONS:\")\n",
    "print(\"    ‚Ä¢ K-Means: Fast, scalable, interpretable (choose for most cases)\")\n",
    "print(\"    ‚Ä¢ Hierarchical: Reveals nested structure (good for exploration)\")\n",
    "print(\"    ‚Ä¢ DBSCAN: Finds outliers, but struggles with uniform density data\")\n",
    "print(\"    ‚Ä¢ ALWAYS validate with multiple metrics and domain knowledge\")\n",
    "print(\"    ‚Ä¢ Scaling is critical for distance-based methods\")\n",
    "print(\"    ‚Ä¢ Cluster count selection is part science, part domain expertise\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION G COMPLETE: Clustering Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  ‚Üí Section H: Visualization Design and Ethics\")\n",
    "print(\"  ‚Üí Section I: Self-Critique and External Visualization Analysis\")\n",
    "print(\"  ‚Üí Section J: Interactive Visualization Tools\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d53409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
