# %% [markdown]
# # Section F: Dimensionality Reduction and PCA
# ## Video Game Sales Dataset - Uncovering Hidden Patterns Through Dimension Reduction

# %% [markdown]
# ### Required Libraries

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import warnings
warnings.filterwarnings('ignore')

# Set visualization defaults
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (16, 8)
sns.set_palette("husl")

# %% [markdown]
# ### Load Dataset

# %%
# Load the dataset
df = pd.read_csv('vgsales.csv')

# Clean Year data
df_clean = df.dropna(subset=['Year'])
df_clean['Year'] = df_clean['Year'].astype(int)

print(f"Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns")
print(f"Clean dataset: {df_clean.shape[0]} rows")
print(f"\nColumns: {df.columns.tolist()}")

# %% [markdown]
# ## 10.1 Motivation and Readiness Check
# 
# ### Core Questions:
# 1. **Is dimensionality reduction appropriate for this dataset?**
# 2. **What information may be lost?**
# 3. **What insights might emerge from reduced dimensions?**

# %% [markdown]
# ### Why Consider Dimensionality Reduction?

# %%
print("ü§î MOTIVATION FOR DIMENSIONALITY REDUCTION")
print("="*70)

print("\n1. DATASET CHARACTERISTICS:")
print(f"   ‚Ä¢ {df.shape[1]} total variables")
print(f"   ‚Ä¢ 4 regional sales columns (NA, EU, JP, Other)")
print(f"   ‚Ä¢ 1 derived column (Global_Sales = sum of regions)")
print(f"   ‚Ä¢ Potential redundancy in regional sales")

print("\n2. ANALYTICAL GOALS:")
print("   ‚Ä¢ Visualize high-dimensional sales patterns in 2D/3D")
print("   ‚Ä¢ Identify latent market structures")
print("   ‚Ä¢ Reduce multicollinearity for future modeling")
print("   ‚Ä¢ Find underlying factors explaining sales variance")

print("\n3. EXPECTED INSIGHTS:")
print("   ‚Ä¢ Are there distinct 'market profiles' (e.g., Western vs Eastern)?")
print("   ‚Ä¢ Can we reduce 4 regional variables to 2-3 components?")
print("   ‚Ä¢ Do natural game clusters emerge in reduced space?")

# %% [markdown]
# ### Data Readiness Assessment

# %%
print("\nüìä DATA READINESS FOR PCA")
print("="*70)

# Select numerical variables for PCA
numerical_cols = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']

# Check for missing values
missing_check = df[numerical_cols].isnull().sum()
print("\n1. Missing Values:")
print(missing_check)
print(f"   ‚Üí All numerical variables complete: ‚úì")

# Check correlations
corr_matrix = df[numerical_cols].corr()
print("\n2. Correlation Matrix (checking for redundancy):")
print(corr_matrix.round(3))

# Identify highly correlated pairs
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        if abs(corr_matrix.iloc[i, j]) > 0.7:
            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))

print("\n3. Highly Correlated Variable Pairs (|r| > 0.7):")
for var1, var2, r in high_corr_pairs:
    print(f"   ‚Ä¢ {var1} ‚Üî {var2}: r = {r:.3f}")

print("\n4. CONCLUSION:")
print("   ‚úì PCA is APPROPRIATE:")
print("     - Regional sales are correlated (redundancy exists)")
print("     - Global_Sales is perfectly redundant (linear combination)")
print("     - Can likely reduce 5 dimensions to 2-3 meaningful components")

# %% [markdown]
# ### What Information Might Be Lost?

# %%
print("\n‚ö†Ô∏è  POTENTIAL INFORMATION LOSS")
print("="*70)

print("\n1. INDIVIDUAL REGIONAL PATTERNS:")
print("   ‚Ä¢ PCA creates linear combinations of regions")
print("   ‚Ä¢ May obscure region-specific anomalies")
print("   ‚Ä¢ Japan's unique market profile might be diluted")

print("\n2. NON-LINEAR RELATIONSHIPS:")
print("   ‚Ä¢ PCA assumes linear combinations")
print("   ‚Ä¢ Non-linear regional interactions will be missed")
print("   ‚Ä¢ Example: 'Strong in JP BUT weak in NA' patterns")

print("\n3. INTERPRETABILITY:")
print("   ‚Ä¢ Principal components are abstract")
print("   ‚Ä¢ Lose direct connection to specific regions")
print("   ‚Ä¢ Business stakeholders may prefer original variables")

print("\n4. OUTLIER INFLUENCE:")
print("   ‚Ä¢ PCA sensitive to extreme values")
print("   ‚Ä¢ Blockbuster games may dominate components")
print("   ‚Ä¢ Typical games may be poorly represented")

print("\n5. CATEGORICAL INFORMATION:")
print("   ‚Ä¢ Genre, Platform, Publisher cannot be directly included")
print("   ‚Ä¢ Must be treated separately or one-hot encoded")
print("   ‚Ä¢ Complex interactions with sales may be lost")

# %% [markdown]
# ### Decision: Proceed with PCA?

# %%
print("\n‚úÖ DECISION: PROCEED WITH PCA")
print("="*70)
print("\nRATIONALE:")
print("  1. High correlation among regional sales justifies reduction")
print("  2. Visualization in 2D/3D will reveal patterns invisible in 5D")
print("  3. Can retain categorical variables for color-coding")
print("  4. Information loss is acceptable for exploratory insights")
print("  5. Will compare results with and without Global_Sales")

print("\nAPPROACH:")
print("  ‚Ä¢ Two PCA variants:")
print("    A) Regional sales only (NA, EU, JP, Other) - 4 dimensions")
print("    B) All sales variables including Global - 5 dimensions")
print("  ‚Ä¢ Standard scaling required (different magnitude ranges)")
print("  ‚Ä¢ Retain components explaining 80-90% variance")

# %% [markdown]
# ---
# ## 10.2 PCA Execution and Interpretation

# %% [markdown]
# ### Feature Scaling Justification

# %%
print("\nüìè FEATURE SCALING NECESSITY")
print("="*70)

# Show original scale differences
print("\nOriginal Variable Scales:")
print(df[numerical_cols].describe().loc[['mean', 'std', 'min', 'max']].round(3))

print("\nüö® WHY SCALING IS MANDATORY:")
print("  1. MAGNITUDE DIFFERENCES:")
print("     ‚Ä¢ NA_Sales ranges 0-40+ million")
print("     ‚Ä¢ Other_Sales ranges 0-10 million")
print("     ‚Ä¢ PCA without scaling would be dominated by larger-scale variables")

print("\n  2. PCA VARIANCE SENSITIVITY:")
print("     ‚Ä¢ PCA identifies directions of maximum variance")
print("     ‚Ä¢ High-magnitude variables artificially inflate variance")
print("     ‚Ä¢ NA_Sales would dominate first PC purely due to scale")

print("\n  3. INTERPRETABILITY:")
print("     ‚Ä¢ Standardized variables have equal weight")
print("     ‚Ä¢ Component loadings become directly comparable")
print("     ‚Ä¢ Mean=0, Std=1 allows fair comparison")

print("\n‚úì SOLUTION: StandardScaler (z-score normalization)")
print("  ‚Üí Transforms each variable to mean=0, variance=1")
print("  ‚Üí Preserves relative relationships")
print("  ‚Üí Ensures all variables contribute fairly")

# %% [markdown]
# ### PCA Variant A: Regional Sales Only (Exclude Global_Sales)

# %%
# Select regional sales (exclude Global_Sales to avoid perfect redundancy)
regional_cols = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']
X_regional = df[regional_cols].values

# Standardize features
scaler_regional = StandardScaler()
X_scaled_regional = scaler_regional.fit_transform(X_regional)

print("üîß PCA VARIANT A: Regional Sales Only")
print("="*70)
print(f"Input dimensions: {X_scaled_regional.shape}")
print(f"Variables: {regional_cols}")
print(f"\nScaled data statistics:")
print(f"  Mean (should be ~0): {X_scaled_regional.mean(axis=0).round(6)}")
print(f"  Std (should be ~1):  {X_scaled_regional.std(axis=0).round(6)}")

# %% [markdown]
# ### Apply PCA

# %%
# Fit PCA with all components
pca_regional = PCA()
X_pca_regional = pca_regional.fit_transform(X_scaled_regional)

# Explained variance
explained_var = pca_regional.explained_variance_ratio_
cumulative_var = np.cumsum(explained_var)

print("\nüìä PCA RESULTS: Explained Variance")
print("="*70)
for i, (var, cum_var) in enumerate(zip(explained_var, cumulative_var), 1):
    print(f"  PC{i}: {var*100:6.2f}% | Cumulative: {cum_var*100:6.2f}%")

# Determine number of components for 90% variance
n_components_90 = np.argmax(cumulative_var >= 0.90) + 1
print(f"\n‚úì Components needed for 90% variance: {n_components_90}")

# %% [markdown]
# ### Visualization: Scree Plot and Cumulative Variance

# %%
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# 1. Scree plot
axes[0].bar(range(1, len(explained_var) + 1), explained_var * 100, 
           alpha=0.7, color='steelblue', edgecolor='black', label='Individual Variance')
axes[0].plot(range(1, len(explained_var) + 1), explained_var * 100, 
            'ro-', linewidth=2, markersize=8, label='Variance Trend')
axes[0].axhline(y=100/len(explained_var), color='red', linestyle='--', 
                linewidth=2, label=f'Average ({100/len(explained_var):.1f}%)')
axes[0].set_xlabel('Principal Component', fontsize=12)
axes[0].set_ylabel('Explained Variance (%)', fontsize=12)
axes[0].set_title('Scree Plot: Variance per Component', fontsize=14, fontweight='bold')
axes[0].set_xticks(range(1, len(explained_var) + 1))
axes[0].legend()
axes[0].grid(alpha=0.3)

# 2. Cumulative variance
axes[1].plot(range(1, len(cumulative_var) + 1), cumulative_var * 100, 
            marker='o', linewidth=2.5, markersize=8, color='darkgreen')
axes[1].axhline(y=90, color='red', linestyle='--', linewidth=2, label='90% Threshold')
axes[1].axhline(y=80, color='orange', linestyle='--', linewidth=2, label='80% Threshold')
axes[1].fill_between(range(1, len(cumulative_var) + 1), cumulative_var * 100, 
                     alpha=0.3, color='green')
axes[1].set_xlabel('Number of Components', fontsize=12)
axes[1].set_ylabel('Cumulative Explained Variance (%)', fontsize=12)
axes[1].set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')
axes[1].set_xticks(range(1, len(cumulative_var) + 1))
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# %% [markdown]
# ### Component Loadings Interpretation

# %%
# Extract loadings (eigenvectors scaled by sqrt of eigenvalues)
loadings = pca_regional.components_.T * np.sqrt(pca_regional.explained_variance_)

loadings_df = pd.DataFrame(
    loadings,
    columns=[f'PC{i+1}' for i in range(len(regional_cols))],
    index=regional_cols
)

print("\nüîç PRINCIPAL COMPONENT LOADINGS")
print("="*70)
print(loadings_df.round(3))

print("\nüìñ INTERPRETATION GUIDE:")
print("  ‚Ä¢ Positive loading: Variable contributes positively to component")
print("  ‚Ä¢ Negative loading: Variable contributes negatively")
print("  ‚Ä¢ Magnitude: Strength of contribution")
print("  ‚Ä¢ Components are orthogonal (uncorrelated)")

# %% [markdown]
# ### Visualization: Loading Heatmap

# %%
plt.figure(figsize=(10, 6))
sns.heatmap(loadings_df.T, annot=True, cmap='RdBu_r', center=0, 
            linewidths=1, cbar_kws={'label': 'Loading Strength'},
            vmin=-1, vmax=1, fmt='.3f', annot_kws={'size': 11, 'weight': 'bold'})
plt.xlabel('Original Variables', fontsize=12)
plt.ylabel('Principal Components', fontsize=12)
plt.title('PCA Loading Matrix: Regional Sales', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### Semantic Interpretation of Components

# %%
print("\nüß† SEMANTIC INTERPRETATION OF PRINCIPAL COMPONENTS")
print("="*70)

print("\nüìç PC1 (Explains {:.1f}%):".format(explained_var[0]*100))
pc1_loadings = loadings_df['PC1'].sort_values(ascending=False)
print(pc1_loadings)
print("\n  INTERPRETATION:")
if all(pc1_loadings > 0):
    print("  ‚Üí 'OVERALL MARKET SIZE' or 'GLOBAL POPULARITY'")
    print("  ‚Üí All regions load positively (global hits sell everywhere)")
    print("  ‚Üí High PC1 = blockbuster game, Low PC1 = niche game")
else:
    print("  ‚Üí Check for regional contrast pattern")

print("\nüìç PC2 (Explains {:.1f}%):".format(explained_var[1]*100))
pc2_loadings = loadings_df['PC2'].sort_values(ascending=False)
print(pc2_loadings)
print("\n  INTERPRETATION:")
if pc2_loadings['JP_Sales'] < 0 and pc2_loadings[['NA_Sales', 'EU_Sales']].mean() > 0:
    print("  ‚Üí 'WESTERN vs EASTERN PREFERENCE'")
    print("  ‚Üí Positive: Western-dominant games (NA + EU strong, JP weak)")
    print("  ‚Üí Negative: Japan-dominant games (JP strong, West weak)")
elif pc2_loadings['NA_Sales'] > 0 and pc2_loadings['EU_Sales'] < 0:
    print("  ‚Üí 'NORTH AMERICA vs EUROPE PREFERENCE'")
else:
    print("  ‚Üí Secondary variance pattern (examine loadings)")

print("\nüìç PC3 (Explains {:.1f}%):".format(explained_var[2]*100))
pc3_loadings = loadings_df['PC3'].sort_values(ascending=False)
print(pc3_loadings)
print("\n  INTERPRETATION:")
print("  ‚Üí Captures remaining regional variance")
print("  ‚Üí Likely 'Other_Sales' contrast or EU-JP differences")
print("  ‚Üí Less interpretable (lower variance explained)")

# %% [markdown]
# ---
## 10.3 PCA-Based Questioning

# %% [markdown]
# ### Question 1: Do Natural Groupings Appear in PC Space?

# %%
# Create DataFrame with PCA coordinates and metadata
df_pca = df.copy()
df_pca['PC1'] = X_pca_regional[:, 0]
df_pca['PC2'] = X_pca_regional[:, 1]
df_pca['PC3'] = X_pca_regional[:, 2]

print("üîç QUESTION 1: Do Natural Groupings Emerge?")
print("="*70)
print("\nWe will visualize games in PC1-PC2 space, colored by:")
print("  1. Genre")
print("  2. Platform")
print("  3. Year era")
print("\nGoal: See if categorical variables align with PC structure")

# %% [markdown]
# ### Visualization: PC1 vs PC2 (Color by Genre)

# %%
# Select top 6 genres for clarity
top_genres = df['Genre'].value_counts().head(6).index.tolist()
df_pca_genres = df_pca[df_pca['Genre'].isin(top_genres)]

plt.figure(figsize=(14, 8))
for genre in top_genres:
    genre_data = df_pca_genres[df_pca_genres['Genre'] == genre]
    plt.scatter(genre_data['PC1'], genre_data['PC2'], 
               alpha=0.6, s=50, label=genre, edgecolors='black', linewidth=0.3)

plt.xlabel(f'PC1 ({explained_var[0]*100:.1f}% variance)', fontsize=12)
plt.ylabel(f'PC2 ({explained_var[1]*100:.1f}% variance)', fontsize=12)
plt.title('PCA Projection: Games Colored by Genre', fontsize=15, fontweight='bold')
plt.legend(title='Genre', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)
plt.axvline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\nüìä OBSERVATION:")
print("  ‚Ä¢ Do genres cluster in specific PC regions?")
print("  ‚Ä¢ Or do they overlap heavily?")

# %% [markdown]
# ### Visualization: PC1 vs PC2 (Color by Platform)

# %%
# Select top 6 platforms
top_platforms = df['Platform'].value_counts().head(6).index.tolist()
df_pca_platforms = df_pca[df_pca['Platform'].isin(top_platforms)]

plt.figure(figsize=(14, 8))
for platform in top_platforms:
    plat_data = df_pca_platforms[df_pca_platforms['Platform'] == platform]
    plt.scatter(plat_data['PC1'], plat_data['PC2'], 
               alpha=0.6, s=50, label=platform, edgecolors='black', linewidth=0.3)

plt.xlabel(f'PC1 ({explained_var[0]*100:.1f}% variance)', fontsize=12)
plt.ylabel(f'PC2 ({explained_var[1]*100:.1f}% variance)', fontsize=12)
plt.title('PCA Projection: Games Colored by Platform', fontsize=15, fontweight='bold')
plt.legend(title='Platform', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)
plt.axvline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\nüìä OBSERVATION:")
print("  ‚Ä¢ Do platforms separate along PC1 or PC2?")
print("  ‚Ä¢ Wii might cluster differently than PS3/X360")

# %% [markdown]
# ### Visualization: PC1 vs PC2 (Color by Year Era)

# %%
# Create year bins
df_pca['Era'] = pd.cut(df_pca['Year'], 
                       bins=[1980, 2000, 2005, 2010, 2020],
                       labels=['Pre-2000', '2000-2005', '2006-2010', '2011-2020'])

plt.figure(figsize=(14, 8))
era_colors = {'Pre-2000': 'purple', '2000-2005': 'blue', '2006-2010': 'green', '2011-2020': 'orange'}

for era in ['Pre-2000', '2000-2005', '2006-2010', '2011-2020']:
    era_data = df_pca[df_pca['Era'] == era]
    plt.scatter(era_data['PC1'], era_data['PC2'], 
               alpha=0.5, s=50, label=era, color=era_colors[era], 
               edgecolors='black', linewidth=0.3)

plt.xlabel(f'PC1 ({explained_var[0]*100:.1f}% variance)', fontsize=12)
plt.ylabel(f'PC2 ({explained_var[1]*100:.1f}% variance)', fontsize=12)
plt.title('PCA Projection: Games Colored by Time Era', fontsize=15, fontweight='bold')
plt.legend(title='Era', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)
plt.axvline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\nüìä OBSERVATION:")
print("  ‚Ä¢ Has market structure shifted over time?")
print("  ‚Ä¢ Do older games occupy different PC space?")

# %% [markdown]
# ### Question 2: Are Patterns Clarified or Obscured?

# %%
print("\n\nüîç QUESTION 2: Does PCA Clarify or Obscure Patterns?")
print("="*70)

print("\nCLARIFICATIONS:")
print("  ‚úì 2D visualization of 4D data (impossible otherwise)")
print("  ‚úì Separation of 'global popularity' (PC1) from 'regional preference' (PC2)")
print("  ‚úì Easier to spot outliers (extreme PC scores)")
print("  ‚úì Reduced noise by focusing on high-variance directions")

print("\nOBSCURATIONS:")
print("  ‚úó Lost direct regional interpretability")
print("  ‚úó Cannot say 'this game sold X in Japan' from PC space")
print("  ‚úó Non-linear patterns (if present) are missed")
print("  ‚úó Categorical variables (Genre, Platform) not integrated")

print("\nVERDICT:")
print("  ‚Üí PCA excels at EXPLORATORY VISUALIZATION")
print("  ‚Üí Useful for hypothesis generation, not definitive answers")
print("  ‚Üí Should complement, not replace, original variable analysis")

# %% [markdown]
# ### Question 3: Which Variables Dominate Variance?

# %%
print("\n\nüîç QUESTION 3: Which Variables Dominate Variance?")
print("="*70)

# Calculate total contribution of each variable across all PCs
total_contribution = (loadings_df**2).sum(axis=1).sort_values(ascending=False)

print("\nTOTAL VARIANCE CONTRIBUTION BY VARIABLE:")
print(total_contribution.round(3))

print("\nINTERPRETATION:")
dominant_var = total_contribution.idxmax()
print(f"  ‚Ä¢ {dominant_var} contributes most to overall variance")
print(f"  ‚Ä¢ This suggests {dominant_var} has the most diverse patterns")

# Visualize contributions
plt.figure(figsize=(10, 6))
total_contribution.plot(kind='bar', color='teal', edgecolor='black', alpha=0.7)
plt.xlabel('Original Variable', fontsize=12)
plt.ylabel('Total Squared Loading (Variance Contribution)', fontsize=12)
plt.title('Variable Contributions to Total PCA Variance', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### Advanced: Biplot (PC Loadings + Data Points)

# %%
# Create biplot: scatter data + loading vectors
fig, ax = plt.subplots(figsize=(14, 10))

# Scatter data points (subsample for clarity)
sample_indices = np.random.choice(len(df_pca), size=min(2000, len(df_pca)), replace=False)
ax.scatter(df_pca.iloc[sample_indices]['PC1'], 
          df_pca.iloc[sample_indices]['PC2'],
          alpha=0.3, s=30, color='lightgray', edgecolors='black', linewidth=0.5,
          label='Games (sample)')

# Plot loading vectors
scaling_factor = 3  # Scale arrows for visibility
for i, var in enumerate(regional_cols):
    ax.arrow(0, 0, 
            loadings_df.loc[var, 'PC1'] * scaling_factor,
            loadings_df.loc[var, 'PC2'] * scaling_factor,
            head_width=0.15, head_length=0.2, fc='red', ec='red', linewidth=2.5)
    ax.text(loadings_df.loc[var, 'PC1'] * scaling_factor * 1.15,
           loadings_df.loc[var, 'PC2'] * scaling_factor * 1.15,
           var, fontsize=13, fontweight='bold', color='darkred',
           ha='center', va='center',
           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))

ax.set_xlabel(f'PC1 ({explained_var[0]*100:.1f}% variance)', fontsize=12)
ax.set_ylabel(f'PC2 ({explained_var[1]*100:.1f}% variance)', fontsize=12)
ax.set_title('PCA Biplot: Data Points + Variable Loadings', fontsize=15, fontweight='bold')
ax.axhline(0, color='black', linestyle='-', linewidth=1, alpha=0.3)
ax.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.3)
ax.legend(loc='upper right')
ax.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\nüìñ BIPLOT INTERPRETATION:")
print("  ‚Ä¢ Arrows show original variable directions in PC space")
print("  ‚Ä¢ Arrow length = strength of contribution to PC1-PC2 plane")
print("  ‚Ä¢ Arrow angle = correlation with PCs")
print("  ‚Ä¢ Games in arrow direction have high values for that variable")

# %% [markdown]
# ---
# ## PCA Variant B: Including Global_Sales (For Comparison)

# %%
print("\nüîß PCA VARIANT B: All Sales Variables (Including Global)")
print("="*70)
print("‚ö†Ô∏è  WARNING: Global_Sales is linear combination of regional sales")
print("   ‚Üí Expect near-perfect correlation with PC1")
print("   ‚Üí Useful to demonstrate redundancy")

# Select all sales variables
all_sales_cols = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']
X_all_sales = df[all_sales_cols].values

# Standardize
scaler_all = StandardScaler()
X_scaled_all = scaler_all.fit_transform(X_all_sales)

# Fit PCA
pca_all = PCA()
X_pca_all = pca_all.fit_transform(X_scaled_all)

# Explained variance
explained_var_all = pca_all.explained_variance_ratio_
cumulative_var_all = np.cumsum(explained_var_all)

print("\nüìä EXPLAINED VARIANCE (All Sales Variables):")
for i, (var, cum_var) in enumerate(zip(explained_var_all, cumulative_var_all), 1):
    print(f"  PC{i}: {var*100:6.2f}% | Cumulative: {cum_var*100:6.2f}%")

# %% [markdown]
# ### Compare Loadings

# %%
loadings_all = pca_all.components_.T * np.sqrt(pca_all.explained_variance_)
loadings_all_df = pd.DataFrame(
    loadings_all,
    columns=[f'PC{i+1}' for i in range(len(all_sales_cols))],
    index=all_sales_cols
)

print("\nüîç LOADINGS: All Sales Variables (Including Global)")
print("="*70)
print(loadings_all_df.round(3))

print("\nüìä OBSERVATION:")
print(f"  ‚Ä¢ PC1 explains {explained_var_all[0]*100:.1f}% of variance")
print(f"  ‚Ä¢ Global_Sales loading on PC1: {loadings_all_df.loc['Global_Sales', 'PC1']:.3f}")
print(f"  ‚Üí Global_Sales dominates PC1 (as expected, it's the sum)")
print(f"  ‚Üí Confirms redundancy: including Global_Sales doesn't add info")

# %% [markdown]
# ### Decision: Use Regional-Only PCA

# %%
print("\n‚úÖ FINAL DECISION: Use Regional-Only PCA (Variant A)")
print("="*70)
print("\nREASONS:")
print("  1. Global_Sales is mathematically redundant")
print("  2. Regional-only PCA gives cleaner component interpretation")
print("  3. PC1 = 'Overall Popularity', PC2 = 'Regional Preference' clearer")
print("  4. Avoids artificial inflation of first component")

print("\nSTANDARD PRACTICE:")
print("  ‚Üí Always remove perfectly collinear features before PCA")
print("  ‚Üí If Y = X1 + X2 + X3, don't include Y in PCA input")

# %% [markdown]
# ---
# ## Advanced: 3D PCA Visualization (Interactive)

# %%
# Create 3D interactive plot with Plotly
df_pca_3d = df_pca[df_pca['Genre'].isin(top_genres)].copy()

fig = px.scatter_3d(
    df_pca_3d,
    x='PC1', y='PC2', z='PC3',
    color='Genre',
    hover_data=['Name', 'Platform', 'Year', 'Global_Sales'],
    title='Interactive 3D PCA: Regional Sales Space',
    labels={
        'PC1': f'PC1 ({explained_var[0]*100:.1f}%)',
        'PC2': f'PC2 ({explained_var[1]*100:.1f}%)',
        'PC3': f'PC3 ({explained_var[2]*100:.1f}%)'
    },
    opacity=0.7,
    size_max=10
)

fig.update_layout(
    scene=dict(
        xaxis_title=f'PC1 ({explained_var[0]*100:.1f}%)',
        yaxis_title=f'PC2 ({explained_var[1]*100:.1f}%)',
        zaxis_title=f'PC3 ({explained_var[2]*100:.1f}%)'
    ),
    width=900,
    height=700
)

fig.show()

print("\nüé® INTERACTIVE 3D EXPLORATION:")
print("  ‚Ä¢ Rotate to view different angles")
print("  ‚Ä¢ Hover over points for game details")
print("  ‚Ä¢ Isolate genres by clicking legend")

# %% [markdown]
# ---
# ## Identifying Outliers in PC Space

# %%
print("\nüîç OUTLIER DETECTION IN PCA SPACE")
print("="*70)

# Calculate Mahalanobis-like distance in PC space (using first 2 PCs)
pc_scores = df_pca[['PC1', 'PC2']].values
distances = np.sqrt(pc_scores[:, 0]**2 + pc_scores[:, 1]**2)
df_pca['PC_Distance'] = distances

# Identify extreme games (top 1%)
threshold = np.percentile(distances, 99)
outliers = df_pca[df_pca['PC_Distance'] > threshold].nlargest(15, 'PC_Distance')

print("\nTOP 15 OUTLIERS IN PC SPACE (Extreme Regional Patterns):")
print("="*70)
print(outliers[['Name', 'Platform', 'Genre', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'PC1', 'PC2', 'PC_Distance']].to_string(index=False))

print("\nüìä INTERPRETATION:")
print("  ‚Ä¢ High PC1, Low PC2: Global blockbusters (sell everywhere)")
print("  ‚Ä¢ Low PC1, High PC2: Western-focused games")
print("  ‚Ä¢ Low PC1, Low PC2: Japan-focused games")
print("  ‚Ä¢ Outliers represent unusual regional sales patterns")

# %% [markdown]
# ### Visualize Outliers

# %%
plt.figure(figsize=(14, 8))

# Plot all games
plt.scatter(df_pca['PC1'], df_pca['PC2'], alpha=0.3, s=30, color='lightgray', label='All Games')

# Highlight outliers
plt.scatter(outliers['PC1'], outliers['PC2'], alpha=0.9, s=150, color='red', 
           edgecolors='black', linewidth=2, label='Outliers (Top 1%)', marker='*')

# Annotate a few
for idx, row in outliers.head(5).iterrows():
    plt.annotate(row['Name'][:20], (row['PC1'], row['PC2']), 
                fontsize=9, color='darkred', fontweight='bold',
                xytext=(10, 10), textcoords='offset points',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                arrowprops=dict(arrowstyle='->', color='red', lw=1.5))

plt.xlabel(f'PC1 ({explained_var[0]*100:.1f}% variance)', fontsize=12)
plt.ylabel(f'PC2 ({explained_var[1]*100:.1f}% variance)', fontsize=12)
plt.title('PCA Space: Highlighting Outlier Games', fontsize=15, fontweight='bold')
plt.legend()
plt.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.3)
plt.axvline(0, color='black', linestyle='--', linewidth=1, alpha=0.3)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# %% [markdown]
# ---
# ## Summary: Section F - PCA Findings

# %% [markdown]
# ### üìä Key PCA Insights

# %%
print("\n" + "="*70)
print("SECTION F SUMMARY: PCA FINDINGS")
print("="*70)

print("\n1. DIMENSIONALITY REDUCTION SUCCESS:")
print(f"   ‚úì Reduced 4 regional dimensions to 2 components")
print(f"   ‚úì Retained {cumulative_var[1]*100:.1f}% of variance")
print(f"   ‚úì First 3 PCs explain {cumulative_var[2]*100:.1f}% of variance")

print("\n2. COMPONENT INTERPRETATIONS:")
print(f"   ‚Ä¢ PC1 ({explained_var[0]*100:.1f}%): 'Overall Market Size'")
print(f"     ‚Üí All regions load positively")
print(f"     ‚Üí High PC1 = global blockbuster, Low PC1 = niche game")
print(f"\n   ‚Ä¢ PC2 ({explained_var[1]*100:.1f}%): 'Regional Preference'")
print(f"     ‚Üí Likely Western vs Eastern contrast")
print(f"     ‚Üí Positive = Western-dominant, Negative = Japan-dominant")
print(f"\n   ‚Ä¢ PC3 ({explained_var[2]*100:.1f}%): 'Secondary Regional Variation'")
print(f"     ‚Üí Captures EU-Other or NA-EU differences")

print("\n3. PATTERNS REVEALED:")
print("   ‚úì Clear separation between global hits and regional favorites")
print("   ‚úì Japan market operates independently (distinct PC2 values)")
print("   ‚úì Western markets (NA + EU) cluster together")
print("   ‚úì Outliers represent unusual regional sales distributions")

print("\n4. INFORMATION RETAINED vs LOST:")
print("   GAINED:")
print("   ‚úì 2D/3D visualization of 4D space")
print("   ‚úì Noise reduction (focus on high-variance directions)")
print("   ‚úì Simplified multicollinearity")
print("\n   LOST:")
print("   ‚úó Direct regional interpretability")
print("   ‚úó Non-linear patterns")
print("   ‚úó Categorical variable integration")

print("\n5. PRACTICAL APPLICATIONS:")
print("   ‚Üí Market segmentation (identify global vs regional games)")
print("   ‚Üí Outlier detection (unusual regional patterns)")
print("   ‚Üí Feature engineering for predictive models")
print("   ‚Üí Visual exploration of high-dimensional relationships")

print("\n6. SCALING JUSTIFICATION VALIDATED:")
print("   ‚úì StandardScaler ensured equal variable contribution")
print("   ‚úì Without scaling, NA_Sales would dominate (largest magnitude)")
print("   ‚úì Components now reflect correlational structure, not scale")

print("\n7. GLOBAL_SALES REDUNDANCY CONFIRMED:")
print("   ‚úì Including Global_Sales doesn't add information")
print("   ‚úì It's a perfect linear combination of regional sales")
print("   ‚úì Regional-only PCA provides cleaner interpretation")

# %% [markdown]
# ### ‚ö†Ô∏è PCA Limitations and Cautions

# %%
print("\n\n‚ö†Ô∏è  PCA LIMITATIONS IN THIS DATASET")
print("="*70)

print("\n1. LINEAR ASSUMPTION:")
print("   ‚Ä¢ PCA assumes linear combinations of variables")
print("   ‚Ä¢ Non-linear regional interactions (e.g., 'JP OR NA but not both') missed")
print("   ‚Ä¢ Consider non-linear methods (t-SNE, UMAP) for complex patterns")

print("\n2. VARIANCE ‚â† IMPORTANCE:")
print("   ‚Ä¢ PCA maximizes variance, not predictive power")
print("   ‚Ä¢ Low-variance features might be important for specific tasks")
print("   ‚Ä¢ Example: Other_Sales has low variance but represents unique markets")

print("\n3. INTERPRETABILITY CHALLENGE:")
print("   ‚Ä¢ Components are abstract mathematical constructs")
print("   ‚Ä¢ Business stakeholders may struggle with 'PC1' terminology")
print("   ‚Ä¢ Always provide semantic labels ('Market Size' > 'PC1')")

print("\n4. OUTLIER SENSITIVITY:")
print("   ‚Ä¢ PCA influenced by extreme values")
print("   ‚Ä¢ Blockbuster games (Wii Sports, GTA V) pull components")
print("   ‚Ä¢ Consider robust PCA or outlier removal for sensitivity analysis")

print("\n5. CATEGORICAL VARIABLES EXCLUDED:")
print("   ‚Ä¢ Genre, Platform, Publisher cannot be directly included")
print("   ‚Ä¢ Must use them as labels, not inputs")
print("   ‚Ä¢ One-hot encoding creates sparsity (not ideal for PCA)")

print("\n6. TEMPORAL DYNAMICS LOST:")
print("   ‚Ä¢ PCA is static snapshot")
print("   ‚Ä¢ Market structure likely evolved over time")
print("   ‚Ä¢ Consider dynamic PCA or era-specific analyses")

# %%
print("\n" + "="*70)
print("SECTION F COMPLETE: Dimensionality Reduction and PCA")
print("="*70)
print("\nKey Achievements:")
print("  ‚úì Justified PCA with correlation analysis")
print("  ‚úì Properly scaled features (StandardScaler)")
print("  ‚úì Interpreted components semantically")
print("  ‚úì Visualized 4D data in 2D/3D")
print("  ‚úì Identified outliers in PC space")
print("  ‚úì Critically evaluated information loss")
print("\nNext Steps:")
print("  ‚Üí Section G: Clustering for Pattern Discovery")
print("  ‚Üí Section H: Visualization Ethics")
print("="*70)

# %%